---
layout: post
title: 'Qwen3_VL_8B'
date: 2025-10-28 17:22 +0800
logs: [AI Challenge]
toc: True
---
## Model: Qwen3
`Qwen3-VL 8B`는 Large Language Model 기반 멀티모달 모델(VLM) 로,
이미지와 텍스트를 함께 입력받아 `시각적 이해` + `언어 추론`을 수행할 수 있도록 설계된 모델이다.

Qwen3-VL 기반 VQA 모델을 구현해본 파이프라인을 바탕으로,
전체 `코드 흐름을 분석`하고 각 구성 요소의 `역할을 정리`하고자 한다. 

### 목적
데이터 로딩부터 학습, 검증, 추론까지 체계적으로 리뷰하여
향후 모델 개선 및 재현성을 높이는 것.

---

### 모듈 설치 및 임포트
```python
!pip install -q transformers accelerate bitsandbytes peft wandb
!pip install -U bitsandbytes
```
설명
- `ransformers`(허깅페이스), `accelerate`(분산/장치 관리), `bitsandbytes`(4bit/8bit 양자화), `peft`(LoRA), `wandb`(로그).
- `bitsandbytes`는 환경에 따라 버전 충돌이 잦아 -U로 최신화.
---

### 기본 임포트 & 환경 설정
```python
import os, re, math, random
import pandas as pd
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from dataclasses import dataclass
import torch
from typing import Any
from transformers import (
    AutoModelForVision2Seq,
    AutoProcessor,
    BitsAndBytesConfig,
    get_linear_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import numpy as np

Image.MAX_IMAGE_PIXELS = None
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)
```
설명
- `AutoModelForVision2Seq`: Qwen3-VL 계열(vision-to-text) 로더.
- `AutoProcessor`: 멀티모달 토크나이저+이미지 전처리.
- `get_linear_schedule_with_warmup`: 선형 감쇠 + 웜업 스케줄러.
- `prepare_model_for_kbit_training`: 저정밀(4bit) 미세튜닝 준비.
- `Image.MAX_IMAGE_PIXELS=None`: 초고해상도 이미지 제한 해제(경고 억제).
- `device`: CUDA 우선.
  
---

### 경로/모델/시드 등 공통 하이퍼파라미터
```python
BASE_DIR = "/content/drive/MyDrive/AI챌린지/2025-ssafy-14/"
MODEL_ID = "Qwen/Qwen3-VL-8B-Instruct"
IMAGE_SIZE = 448
MAX_NEW_TOKENS = 8
SEED = 42
random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)
```
주요 파라미터
- `MODEL_ID: Qwen3-VL-8B Instruct` 멀티모달.
- `IMAGE_SIZE=448`: 입력 픽셀 스케일(메모리/속도·성능 절충선).
- `MAX_NEW_TOKENS=8`: VQA MC는 한 글자 답 출력을 강제하므로 매우 작게.
- 고정 시드로 재현성 확보, Colab/Drive 경로 고정
  
---

### 4bit 양자화 + Processor 준비 + LoRA구성
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

processor = AutoProcessor.from_pretrained(
    MODEL_ID,
    min_pixels=IMAGE_SIZE*IMAGE_SIZE,
    max_pixels=IMAGE_SIZE*IMAGE_SIZE,
    trust_remote_code=True,
)

base_model = AutoModelForVision2Seq.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

base_model = prepare_model_for_kbit_training(base_model)
base_model.gradient_checkpointing_enable()

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
    task_type="CAUSAL_LM",
)
model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()
```
주요 파라미터/전략
- `4bit(NF4)`: 메모리 절감(8B 모델 학습 가능), double_quant로 추가 압축.
- `compute_dtype=float16`: 연산 정밀도. (A100이면 bfloat16 고려 가능)
- `min/max_pixels`: 리사이즈 스케일 고정(=448²), 배치 안정.
- `gradient_checkpointing_enable()`: 활성 메모리 감소 ↔ 연산량 증가(속도 저하 수용).
- `LoRA: r=16, alpha=32, drop=0.05` **attention/MLP 전체(target_modules)**에 삽입 → 표현력/안정성 균형.
- `target_modules`리스트에 있는 프로젝션 레이어 이름은 모델마다 명칭이 다름!!(현재 Qwen3-VL에 맞춤).
- `task_type='CAUSAL_LM'` 자귀회귀 언어 모델링: 앞의 토큰을 보고 다음 토큰을 예측하는 형태의 학습 
  
---

### 시스템 프롬프트 & 문제 프롬프트 빌더
```python
SYSTEM_INSTRUCT = (
    "당신은 시각 정보를 세밀하게 분석하여 질문에 논리적으로 답하는 비주얼 AI 전문가입니다. "
    "이미지에서 주요 대상과 그 특징(형태, 재질, 위치, 배경)을 관찰하고, "
    "보기 중 가장 가능성이 높은 정답을 선택하세요. "
    "최종 출력은 반드시 a, b, c, d 중 하나의 소문자 한 글자만 작성하세요."
)

def build_mc_prompt(question, a, b, c, d, task_hint):
    base_prompt = (
        f"질문: {question}\n\n"
        f"보기:\n(a) {a}\n(b) {b}\n(c) {c}\n(d) {d}\n\n"
        "이미지를 관찰한 후, 가장 알맞은 보기를 고르세요. "
        "정답은 반드시 소문자 알파벳 a, b, c, d 중 하나로만 답하세요."
    )
    base_prompt += f"\n\n힌트: {task_hint}"
    return base_prompt
```
- 출력 형식 강제로 디코딩 일관성 확보
  
---

### 데이터셋 클래스(멀티모달 메시지 생성)
```python
class VQAMCDataset(Dataset):
    def __init__(self, df, processor, train=True):
        self.df = df.reset_index(drop=True)
        self.processor = processor
        self.train = train
        self.path = '/content/drive/MyDrive/AI챌린지/2025-ssafy-14/'

    def __len__(self): return len(self.df)

    def __getitem__(self, i):
        row = self.df.iloc[i]
        img = Image.open(self.path + row["path"]).convert("RGB")

        q = str(row["question"])
        a, b, c, d, e = str(row["a"]), str(row["b"]), str(row["c"]), str(row["d"]), str(row['task_hint'])
        user_text = build_mc_prompt(q, a, b, c, d, e)

        # ✅ Qwen3는 멀티모달 메시지 기반이므로 구조 그대로 유지 가능
        messages = [
            {"role": "system", "content": [{"type": "text", "text": SYSTEM_INSTRUCT}]},
            {"role": "user", "content": [
                {"type": "image", "image": img},
                {"type": "text", "text": user_text}
            ]}
        ]
        if self.train:
            gold = str(row["answer"]).strip().lower()
            messages.append({"role": "assistant", "content": [{"type": "text", "text": gold}]})

        return {"messages": messages, "image": img}
```
역할
- 한 샘플을 Qwen3-VL chat 포맷으로 구성(system/user/assistant)
- 학습 시 teacher forcing을 위해 gold 정답을 assistant로 추가(SFT)
  
---

### 데이터 콜레이터(토크나이즈/ 이미지 패킹)
```python
from dataclasses import dataclass

@dataclass
class DataCollator:
    processor: Any
    train: bool = True

    def __call__(self, batch):
        texts, images = [], []
        for sample in batch:
            messages = sample["messages"]
            img = sample["image"]

            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            texts.append(text)
            images.append(img)

        enc = self.processor(
            text=texts,
            images=images,
            padding=True,
            return_tensors="pt"
        )

        if self.train:
            enc["labels"] = enc["input_ids"].clone()

        return enc
```
역할
- Dataset에서 가져온 개별 샘플들을 →
모델이 바로 입력할 수 있는 하나의 batch로 묶어주는 역할
- Dataset().__getitem__()은 아래와 같이 주어짐
  ```python
   { 
     "messages":  ,
     "image":  ,
   }
  ```
- 하지만 모델은 아래 형태를 기대함
  ```python
   { 
     "input_ids":  ,
     "pixel_values":  ,
     "labels":  ,
   }
  ```
- 즉 데이터 콜레이터는 `토크나이즈`, `이미지 변환`, `패딩`, `라벨 세팅` 등을 통일된 방식으로 자동 처리해줌으로써 멀티모달 구조 처리가 가능하게 하도록 함 

  **한 줄 요약**\
  DataCollator는
  개별 샘플 → 모델 입력(batch)로 변환하는 최종 전처리 단계이며
  멀티모달/Chat 형식에서는 특히 필수적인 구성 요소다.

---

### Stratified Split + DataLoader
```python
# 1) stratify용 라벨 준비
labels = train_df["task_type"].astype(str)
labels = labels.replace({"nan": np.nan}).fillna("unknown")  # NaN도 unknown으로

# 2) 희귀 클래스(1개뿐인 클래스)는 unknown으로 흡수
vc = labels.value_counts()
rare_classes = set(vc[vc < 2].index)

labels_mapped = labels.copy()
labels_mapped = np.where(labels.isin(rare_classes), "unknown", labels)

# 3) stratify split
train_subset, valid_subset = train_test_split(
    train_df,
    test_size=0.10,
    random_state=42,
    shuffle=True,
    stratify=labels_mapped  # 희귀 클래스 통합 버전
)

# 4) Dataset / DataLoader 구성
train_ds = VQAMCDataset(train_subset.reset_index(drop=True), processor, train=True)
valid_ds = VQAMCDataset(valid_subset.reset_index(drop=True), processor, train=True)

train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=DataCollator(processor, True))
valid_loader = DataLoader(valid_ds, batch_size=1, shuffle=False, collate_fn=DataCollator(processor, True))
```
- stratify로 `검증의 편향 최소화`
- 주의: valid_ds에서 train=True는 loss 계산을 위해 gold 포함(의도됨)
- `배치 1`: 8B + 4bit + 이미지 입력에서 OOM 방지
  
---

### W&B 초기화(실험 메타데이터/로그)
```python
# 라이브러리 설치 및 임포트
!pip install wandb -q
import wandb
wandb.login()

import math
import torch
from tqdm.auto import tqdm
from transformers import get_linear_schedule_with_warmup
# 2. WandB 초기화
wandb.init(
    project="SSAFY_AI_CHALLENGE",       # 팀 프로젝트 이름
    entity="ssafy_A014",       # 팀 엔터티 (팀 워크스페이스 이름)
    name="best_qwen3_8b_3epoch_typeprompt",    # 실험 이름 (원하면 동적으로 변경 가능)
    # 실험 파라미터 설정
    config={
        "model": "Qwen3-VL-8B-LoRA",  # 모델 이름
        "epochs": 3,
        # "batch_size": 16,
        "grad_accum": 4,
        "lr": 1e-4,
        "scheduler": "linear_warmup_3%",
        "precision": "bfloat16",
    }
)
```
- 프로젝트/엔터티/실험명으로 대시보드 정리
- 주요 하이퍼파라미터 기록으로 재현성↑
- 주의: Colab/서버는 wandb.login() 토큰 필요
  
---

### 정답 문자 추출기
```python
def extract_choice(text: str) -> str:
    text = text.strip().lower()

    lines = [l.strip() for l in text.splitlines() if l.strip()]
    if not lines:
        return "a"
    last = lines[-1]
    if last in ["a", "b", "c", "d"]:
        return last

    tokens = last.split()
    for tok in tokens:
        if tok in ["a", "b", "c", "d"]:
            return tok
    return "a"
```
 실패 시 보수적으로 "a" 반환 : 점수 손실 최소화 
---

### 학습/검증 루프 
```python
# 3. 설정
config = wandb.config
device = "cuda"

model = model.to(device)
GRAD_ACCUM = config.grad_accum
optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)
num_training_steps = config.epochs * math.ceil(len(train_loader)/GRAD_ACCUM)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    int(num_training_steps*0.03),
    num_training_steps
)
scaler = torch.cuda.amp.GradScaler(enabled=True)

# 4. 학습 루프
global_step = 0
all_outputs = []
for epoch in range(config.epochs):
    running = 0.0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} [train]", unit="batch")
    for step, batch in enumerate(progress_bar, start=1):
        batch = {k:v.to(device) for k,v in batch.items()}

        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
            outputs = model(**batch)
            loss = outputs.loss / GRAD_ACCUM

        scaler.scale(loss).backward()
        running += loss.item()

        if step % GRAD_ACCUM == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)
            scheduler.step()
            global_step += 1

            avg_loss = running / GRAD_ACCUM
            progress_bar.set_postfix({"loss": f"{avg_loss:.3f}"})
            running = 0.0

            # WandB 로그 기록
            wandb.log({
                "train/loss": avg_loss,
                "train/learning_rate": scheduler.get_last_lr()[0],
                "global_step": global_step,
                "epoch": epoch + (step / len(train_loader))
            })

    # Validation
    model.eval()
    val_loss = 0.0
    val_steps = 0

    # 해당 샘플의 id, 모델이 예측한 정답, 실제 정답 라벨
    val_ids, val_preds, val_golds = [], [], []

    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):
        for vstep, vb in enumerate(tqdm(valid_loader, desc=f"Epoch {epoch+1} [valid]", unit="batch"), start=1):
            vb = {k:v.to(device) for k,v in vb.items()}
            loss = model(**vb).loss.item()
            val_loss += loss
            val_steps += 1

            # <-- NEW: test와 동일한 방식으로 생성 기반 예측 (chat template + extract_choice)
            row = valid_subset.iloc[vstep - 1]
            img_path = BASE_DIR + row["path"] if not str(row["path"]).startswith("/") else row["path"]
            img = Image.open(img_path).convert("RGB")

            user_text = build_mc_prompt(row["question"], row["a"], row["b"], row["c"], row["d"], row['task_hint'])
            messages = [
                {"role": "system", "content": [{"type": "text", "text": SYSTEM_INSTRUCT}]},
                {"role": "user", "content": [
                    {"type": "image", "image": img},
                    {"type": "text", "text": user_text}
                ]}
            ]

            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text], images=[img], return_tensors="pt").to(device)

            out_ids = model.generate(
                **inputs,
                max_new_tokens=2,
                do_sample=False,
                eos_token_id=processor.tokenizer.eos_token_id
            )
            output_text = processor.batch_decode(out_ids, skip_special_tokens=True)[0]
            pred = extract_choice(output_text)

            # <-- NEW: gold는 valid_subset에서 직접 참조 (순서 일치 가정)
            gold = str(valid_subset.iloc[vstep-1]["answer"]).strip().lower()
            val_preds.append(pred)
            val_golds.append(gold)

            val_ids.append(valid_subset.iloc[vstep-1]["id"])


            # Validation도 실시간으로 step별 기록
            wandb.log({
                "valid/loss": loss,
                "step": global_step + vstep,  # train과 같은 축에서 표시
                "epoch": epoch + (vstep / len(valid_loader))
            })

    avg_val_loss = val_loss / val_steps
    print(f"[Epoch {epoch+1}] valid loss {avg_val_loss:.4f}")

    # <-- NEW: accuracy 계산 및 로깅/저장
    correct = sum(p == g for p, g in zip(val_preds, val_golds))
    val_acc = correct / max(len(val_golds), 1)
    print(f"[Epoch {epoch+1}] valid accuracy {val_acc:.4f}")

    # Validation 로그 기록
    wandb.log({
        "valid/loss": avg_val_loss,
        "epoch": epoch + 1
    })

    # dataframe으로 보기(output_df)
    cols = {}
    if len(val_ids) == len(val_preds) and len(val_ids) > 0:
        cols["id"] = val_ids
    cols["gold"] = val_golds
    cols["pred"] = val_preds
    output_df = pd.DataFrame(cols)

    output_df["epoch"] = epoch + 1
    all_outputs.append(output_df)

    print(f"총 {len(output_df)}개 샘플 중 {correct}개 정답 ({val_acc*100:.2f}% 정확도)")

    model.train()

# 5. 모델 저장 및 업로드
all_output_df = pd.concat(all_outputs, ignore_index=True)

SAVE_DIR = "/content/drive/MyDrive/AI챌린지/2025-ssafy-14/best_qwen3_8b_3epochs_typeprompt"
model.save_pretrained(SAVE_DIR)
processor.save_pretrained(SAVE_DIR)
print("Saved:", SAVE_DIR)

# 모델 파일 WandB에 업로드 (선택)
wandb.save(SAVE_DIR + "/*")
wandb.finish()
```
- `Grad Accumulation`(그래디언트 누적)의 역할
   - 메모리가 부족해서 큰 배치를 못 쓰는 상황에서, 여러 번의 작은 배치로 역전파를 누적하여 유효 배치 크기를 키우는 기법.
- `num_training_steps`
   - 전체 optimizer step 수 = (에폭 수) × (한 에폭당 optimizer step 수).
   - 이 값은 스케줄러 스케줄을 만들 때 기준이 됨 
   - `num_training_steps * 0.03` : 총 step 중 앞 3%는 천천히 학습률을 주다가 이후에는 선형 감쇠를 준다는 의미 
 - Optimizer / Scheduler 호출 순서
   1. scaler.step(optimizer) (또는 optimizer.step())
   2. scaler.update() (fp16인 경우)
   3. optimizer.zero_grad(set_to_none=True) → 다음 누적을 위해 그래디언트 비움
   4. scheduler.step() → optimizer step마다 1 스텝 전진
   5. global_step += 1 → 실제 업데이트 횟수 카운트

---

### 추론(테스트)
```python
# 추론을 위해 모든 레이어 활성화
model.eval()
preds = []

# 추론 루프
for i in tqdm(range(len(test_df)), desc="Inference", unit="sample"):
    row = test_df.iloc[i]
    test_path = "/content/drive/MyDrive/AI챌린지/2025-ssafy-14/" + row["path"]
    img = Image.open(test_path).convert("RGB")
    user_text = build_mc_prompt(row["question"], row["a"], row["b"], row["c"], row["d"], row["task_hint"])

    messages = [
        {"role":"system","content":[{"type":"text","text":SYSTEM_INSTRUCT}]},
        {"role":"user","content":[
            {"type":"image","image":img},
            {"type":"text","text":user_text}
        ]}
    ]

    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[img], return_tensors="pt").to(device)

    with torch.no_grad():
        out_ids = model.generate(**inputs, max_new_tokens=2, do_sample=False,
                                 eos_token_id=processor.tokenizer.eos_token_id)
    output_text = processor.batch_decode(out_ids, skip_special_tokens=True)[0]

    preds.append(extract_choice(output_text))

# 제출 파일 생성
submission = pd.DataFrame({"id": test_df["id"], "answer": preds})
submission.to_csv("/content/drive/MyDrive/AI챌린지/2025-ssafy-14/best_qwen3_8b_3epochs_typeprompt.csv", index=False)
print("Saved ")
```
 - 학습과 동일한 chat template + generate 경로로 일관 추론.
 - max_new_tokens=2, do_sample=False(deterministic)로 재현성 및 형식 안정.
 - extract_choice로 후처리 일원화.


